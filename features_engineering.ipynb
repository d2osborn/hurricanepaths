{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d873af",
   "metadata": {},
   "source": [
    "# Feature Engineering for Kalman Filter\n",
    "\n",
    "Prepare data for hurricane track prediction using Kalman filter state-space model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4567d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b56500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ibtracs(path=\"ibtracs.ALL.list.v04r01.csv\"):\n",
    "    df = pd.read_csv(path, skiprows=[1], low_memory=False)\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    df = df.replace(' ', np.nan)\n",
    "    \n",
    "    df['storm_dir'] = pd.to_numeric(df['storm_dir'], errors='coerce').astype('Int64')\n",
    "    df['storm_speed'] = pd.to_numeric(df['storm_speed'], errors='coerce').astype('Int64')\n",
    "    df['usa_wind'] = pd.to_numeric(df['usa_wind'], errors='coerce').astype('Int64')\n",
    "    df['usa_lat'] = pd.to_numeric(df['usa_lat'], errors='coerce').astype('Float64')\n",
    "    df['usa_lon'] = pd.to_numeric(df['usa_lon'], errors='coerce').astype('Float64')\n",
    "    df['iso_time'] = pd.to_datetime(df['iso_time'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_ibtracs(\"ibtracs.ALL.list.v04r01.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c00f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to storms with sufficient observations for Kalman filter\n",
    "obs_per_storm = df.groupby('sid').size()\n",
    "valid_storms = obs_per_storm[obs_per_storm >= 2].index\n",
    "df = df[df['sid'].isin(valid_storms)].copy()\n",
    "df = df.sort_values(['sid', 'iso_time']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "571470d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_velocity_from_positions(storm_data):\n",
    "    \"\"\"Compute velocity from position differences using haversine distance\"\"\"\n",
    "    storm_data = storm_data.sort_values('iso_time').copy()\n",
    "    \n",
    "    dt = storm_data['iso_time'].diff().dt.total_seconds() / 3600  # hours\n",
    "    dlat = storm_data['lat'].diff()\n",
    "    dlon = storm_data['lon'].diff()\n",
    "    \n",
    "    # Handle longitude wrapping at Â±180\n",
    "    dlon = dlon.copy()\n",
    "    dlon[dlon > 180] -= 360\n",
    "    dlon[dlon < -180] += 360\n",
    "    \n",
    "    # Convert degrees to km (approximate)\n",
    "    lat_km = dlat * 111.0\n",
    "    lon_km = dlon * 111.0 * np.cos(np.radians(storm_data['lat']))\n",
    "    \n",
    "    # Compute speed (km/h) and direction (degrees)\n",
    "    speed_kmh = np.sqrt(lat_km**2 + lon_km**2) / dt.replace(0, np.nan)\n",
    "    direction = np.degrees(np.arctan2(lon_km, lat_km)) % 360\n",
    "    \n",
    "    # Convert speed to knots (1 knot = 1.852 km/h)\n",
    "    speed_knots = speed_kmh / 1.852\n",
    "    \n",
    "    return speed_knots, direction\n",
    "\n",
    "# Fill missing velocity for storms that need it\n",
    "for sid in df['sid'].unique():\n",
    "    storm = df[df['sid'] == sid]\n",
    "    missing_mask = storm['storm_speed'].isna() | storm['storm_dir'].isna()\n",
    "    \n",
    "    if missing_mask.any():\n",
    "        speed, direction = compute_velocity_from_positions(storm)\n",
    "        df.loc[storm.index[missing_mask], 'storm_speed'] = speed[missing_mask].values\n",
    "        df.loc[storm.index[missing_mask], 'storm_dir'] = direction[missing_mask].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128c2d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert positions and velocities to metric coordinates (km) for Kalman filter\n",
    "# This ensures unit consistency: state vector in km, velocities in km/6h\n",
    "\n",
    "# Earth radius in km (approximate)\n",
    "R_EARTH = 111.0  # km per degree latitude\n",
    "\n",
    "def convert_to_metric_coordinates(df_storm):\n",
    "    \"\"\"Convert lat/lon to metric (x, y) coordinates in km relative to storm start\"\"\"\n",
    "    df_storm = df_storm.sort_values('iso_time').copy()\n",
    "    \n",
    "    # Use first position as reference point\n",
    "    lat_ref = df_storm['lat'].iloc[0]\n",
    "    lon_ref = df_storm['lon'].iloc[0]\n",
    "    \n",
    "    # Convert to metric coordinates\n",
    "    # x: east-west (longitude direction) in km\n",
    "    # y: north-south (latitude direction) in km\n",
    "    lat_rad = np.radians(df_storm['lat'])\n",
    "    lon_rad = np.radians(df_storm['lon'])\n",
    "    lat_ref_rad = np.radians(lat_ref)\n",
    "    lon_ref_rad = np.radians(lon_ref)\n",
    "    \n",
    "    # Haversine-based conversion\n",
    "    dlat = lat_rad - lat_ref_rad\n",
    "    dlon = lon_rad - lon_ref_rad\n",
    "    \n",
    "    # Approximate metric coordinates (valid for small distances)\n",
    "    # More accurate than simple scaling, accounts for latitude\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat_ref_rad) * np.cos(lat_rad) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    distance = R_EARTH * np.degrees(c)\n",
    "    \n",
    "    # Direction from reference\n",
    "    y_km = R_EARTH * np.degrees(dlat)\n",
    "    x_km = R_EARTH * np.cos(lat_ref_rad) * np.degrees(dlon)\n",
    "    \n",
    "    return x_km.values, y_km.values, lat_ref, lon_ref\n",
    "\n",
    "# Convert positions to metric coordinates for each storm\n",
    "df['x_km'] = np.nan\n",
    "df['y_km'] = np.nan\n",
    "df['lat_ref'] = np.nan\n",
    "df['lon_ref'] = np.nan\n",
    "\n",
    "for sid in df['sid'].unique():\n",
    "    storm_idx = df[df['sid'] == sid].index\n",
    "    x_km, y_km, lat_ref, lon_ref = convert_to_metric_coordinates(df.loc[storm_idx])\n",
    "    df.loc[storm_idx, 'x_km'] = x_km\n",
    "    df.loc[storm_idx, 'y_km'] = y_km\n",
    "    df.loc[storm_idx, 'lat_ref'] = lat_ref\n",
    "    df.loc[storm_idx, 'lon_ref'] = lon_ref\n",
    "\n",
    "# Compute velocities in km per 6 hours\n",
    "# Calculate velocity as change in position per 6-hour period\n",
    "def compute_metric_velocity(storm_data):\n",
    "    \"\"\"Compute velocity in km/6h from position changes\"\"\"\n",
    "    storm_data = storm_data.sort_values('iso_time').copy()\n",
    "    \n",
    "    dt_hours = storm_data['iso_time'].diff().dt.total_seconds() / 3600\n",
    "    dx_km = storm_data['x_km'].diff()\n",
    "    dy_km = storm_data['y_km'].diff()\n",
    "    \n",
    "    # Convert to velocity in km per 6 hours\n",
    "    # Normalize by actual time difference, then scale to 6-hour units\n",
    "    vx_km_6h = (dx_km / dt_hours.replace(0, np.nan)) * 6\n",
    "    vy_km_6h = (dy_km / dt_hours.replace(0, np.nan)) * 6\n",
    "    \n",
    "    # Fill first NaN with storm_speed if available\n",
    "    if pd.isna(vx_km_6h.iloc[0]) and not pd.isna(storm_data['storm_speed'].iloc[0]):\n",
    "        speed_kmh = storm_data['storm_speed'].iloc[0] * 1.852  # knots to km/h\n",
    "        speed_km_6h = speed_kmh * 6\n",
    "        direction = np.radians(storm_data['storm_dir'].iloc[0])\n",
    "        vx_km_6h.iloc[0] = speed_km_6h * np.sin(direction)\n",
    "        vy_km_6h.iloc[0] = speed_km_6h * np.cos(direction)\n",
    "    \n",
    "    return vx_km_6h.values, vy_km_6h.values\n",
    "\n",
    "for sid in df['sid'].unique():\n",
    "    storm_idx = df[df['sid'] == sid].index\n",
    "    vx_km, vy_km = compute_metric_velocity(df.loc[storm_idx])\n",
    "    df.loc[storm_idx, 'vx_km'] = vx_km\n",
    "    df.loc[storm_idx, 'vy_km'] = vy_km\n",
    "\n",
    "# Also keep degree-based velocities for backward compatibility (will be deprecated)\n",
    "df['v_lat'] = df['storm_speed'] * 1.852 * 6 / 111.0 * np.cos(np.radians(df['storm_dir']))\n",
    "df['v_lon'] = df['storm_speed'] * 1.852 * 6 / (111.0 * np.cos(np.radians(df['lat']))) * np.sin(np.radians(df['storm_dir']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd15884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features\n",
    "df['storm_age_hours'] = df.groupby('sid')['iso_time'].transform(lambda x: (x - x.min()).dt.total_seconds() / 3600)\n",
    "df['day_of_year'] = df['iso_time'].dt.dayofyear\n",
    "df['month'] = df['iso_time'].dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b42f32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute acceleration (change in velocity) for enhanced state representation\n",
    "def compute_acceleration(storm_data):\n",
    "    storm_data = storm_data.sort_values('iso_time').copy()\n",
    "    dt = storm_data['iso_time'].diff().dt.total_seconds() / 3600\n",
    "    \n",
    "    dv_lat = storm_data['v_lat'].diff() / dt.replace(0, np.nan)\n",
    "    dv_lon = storm_data['v_lon'].diff() / dt.replace(0, np.nan)\n",
    "    \n",
    "    return dv_lat, dv_lon\n",
    "\n",
    "for sid in df['sid'].unique():\n",
    "    storm_idx = df[df['sid'] == sid].index\n",
    "    dv_lat, dv_lon = compute_acceleration(df.loc[storm_idx])\n",
    "    df.loc[storm_idx, 'a_lat'] = dv_lat.values\n",
    "    df.loc[storm_idx, 'a_lon'] = dv_lon.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "531e8e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full feature df -> hurricane_paths_processed_FULL.pkl\n",
      "Saved clean modeling df -> hurricane_paths_processed_CLEAN.pkl\n",
      "Saved modeling-only df -> hurricane_paths_processed_MODEL.pkl\n",
      "\n",
      "State cols: ['sid', 'iso_time', 'lat', 'lon', 'v_lat', 'v_lon', 'storm_speed', 'storm_dir', 'x_km', 'y_km', 'vx_km', 'vy_km', 'lat_ref', 'lon_ref', 'usa_wind']\n",
      "Metric cols present: ['x_km', 'y_km', 'vx_km', 'vy_km', 'lat_ref', 'lon_ref']\n",
      "df_clean rows: 721,960\n",
      "df_model_clean rows: 170,106\n",
      "storms total: 13,450\n",
      "storms modeling: 6,360\n"
     ]
    }
   ],
   "source": [
    "# Track curvature: measures how sharply storm is turning (indicates when linear assumption breaks)\n",
    "def compute_track_curvature(storm_data):\n",
    "    storm_data = storm_data.sort_values('iso_time').copy()\n",
    "    \n",
    "    ddir = storm_data['storm_dir'].diff()\n",
    "    ddir = ddir.copy()\n",
    "    ddir[ddir > 180] -= 360\n",
    "    ddir[ddir < -180] += 360\n",
    "    ddir = ddir.abs()\n",
    "    \n",
    "    speed = storm_data['storm_speed']\n",
    "    curvature = ddir / (speed.replace(0, np.nan) + 1e-6)\n",
    "    \n",
    "    return curvature\n",
    "\n",
    "for sid in df['sid'].unique():\n",
    "    storm_idx = df[df['sid'] == sid].index\n",
    "    df.loc[storm_idx, 'track_curvature'] = compute_track_curvature(df.loc[storm_idx]).values\n",
    "\n",
    "# Latitude regime: affects storm motion characteristics\n",
    "df['latitude_regime'] = pd.cut(df['lat'].abs(), bins=[0, 18, 28, 90], labels=[0, 1, 2], right=False).astype(float)\n",
    "\n",
    "# Hemisphere indicator\n",
    "df['hemisphere'] = (df['lat'] > 0).astype(int)\n",
    "\n",
    "# Motion regime classification based on direction and speed\n",
    "def classify_motion_regime(row):\n",
    "    if pd.isna(row['storm_dir']) or pd.isna(row['storm_speed']):\n",
    "        return np.nan\n",
    "    \n",
    "    speed = row['storm_speed']\n",
    "    direction = row['storm_dir']\n",
    "    \n",
    "    if speed < 5:\n",
    "        return 2\n",
    "    elif 225 <= direction <= 315 or (direction <= 45) or (direction >= 315):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "df['motion_regime'] = df.apply(classify_motion_regime, axis=1)\n",
    "\n",
    "# Storm stage encoding from nature column\n",
    "def encode_storm_stage(nature):\n",
    "    if pd.isna(nature):\n",
    "        return np.nan\n",
    "    mapping = {'DS': 0, 'NR': 1, 'SS': 1, 'TS': 2, 'MX': 3, 'ET': 4}\n",
    "    return mapping.get(nature, np.nan)\n",
    "\n",
    "if 'nature' in df.columns:\n",
    "    df['storm_stage'] = df['nature'].apply(encode_storm_stage)\n",
    "\n",
    "# Landfall proximity features\n",
    "if 'dist2land' in df.columns:\n",
    "    df['is_approaching_land'] = (df['dist2land'] < 200).astype(int)\n",
    "    df['land_gradient'] = df.groupby('sid')['dist2land'].diff() * -1  # Negative means approaching\n",
    "\n",
    "# Beta-drift proxy: Coriolis-related drift (approximation)\n",
    "df['beta_drift_lat'] = df['hemisphere'] * np.sin(np.radians(df['lat'].abs())) * 0.1\n",
    "df['beta_drift_lon'] = df['hemisphere'] * np.sign(df['lat']) * np.cos(np.radians(df['lat'].abs())) * 0.05\n",
    "\n",
    "# Smoothed velocity using 3-point moving average (reduces noise)\n",
    "df['v_lat_smooth'] = df.groupby('sid')['v_lat'].transform(lambda x: x.rolling(window=3, center=True, min_periods=1).mean())\n",
    "df['v_lon_smooth'] = df.groupby('sid')['v_lon'].transform(lambda x: x.rolling(window=3, center=True, min_periods=1).mean())\n",
    "\n",
    "# Autoregressive motion features: recent motion averages\n",
    "df['v_lat_avg_6h'] = df.groupby('sid')['v_lat'].transform(lambda x: x.rolling(window=2, min_periods=1).mean())\n",
    "df['v_lon_avg_6h'] = df.groupby('sid')['v_lon'].transform(lambda x: x.rolling(window=2, min_periods=1).mean())\n",
    "df['v_lat_avg_12h'] = df.groupby('sid')['v_lat'].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "df['v_lon_avg_12h'] = df.groupby('sid')['v_lon'].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "\n",
    "# Ensure agency columns exist (merge raw if missing)\n",
    "needed_agency_cols = [\"usa_agency\", \"wmo_agency\", \"usa_lat\", \"usa_lon\", \"basin\"]\n",
    "missing_agency_info = [c for c in needed_agency_cols if c not in df.columns]\n",
    "\n",
    "if missing_agency_info:\n",
    "    print(f\"Missing agency cols: {missing_agency_info}\")\n",
    "    print(\"Loading raw IBTrACS to merge agency info...\")\n",
    "    \n",
    "    def load_ibtracs_raw(path=\"ibtracs.ALL.list.v04r01.csv\"):\n",
    "        raw = pd.read_csv(path, skiprows=[1], low_memory=False)\n",
    "        raw.columns = [c.lower() for c in raw.columns]\n",
    "        raw = raw.replace(\" \", np.nan)\n",
    "        raw[\"iso_time\"] = pd.to_datetime(raw[\"iso_time\"])\n",
    "        return raw\n",
    "    \n",
    "    df_raw = load_ibtracs_raw(\"ibtracs.ALL.list.v04r01.csv\")\n",
    "    pull_cols = [\"sid\", \"iso_time\", \"basin\", \"subbasin\", \"usa_agency\", \"wmo_agency\", \"usa_lat\", \"usa_lon\"]\n",
    "    pull_cols = [c for c in pull_cols if c in df_raw.columns]\n",
    "    df_agency = df_raw[pull_cols].copy()\n",
    "    \n",
    "    df = df.merge(df_agency, on=[\"sid\", \"iso_time\"], how=\"left\", suffixes=(\"\", \"_raw\"))\n",
    "    if \"basin_raw\" in df.columns:\n",
    "        df[\"basin\"] = df[\"basin\"].fillna(df[\"basin_raw\"])\n",
    "        df.drop(columns=[\"basin_raw\"], inplace=True)\n",
    "    if \"subbasin_raw\" in df.columns and \"subbasin\" not in df.columns:\n",
    "        df.rename(columns={\"subbasin_raw\": \"subbasin\"}, inplace=True)\n",
    "\n",
    "# Fill missing basin using subbasin\n",
    "if \"subbasin\" in df.columns:\n",
    "    sub_to_basin = {\"BB\": \"NI\", \"AS\": \"NI\", \"CP\": \"WP\", \"MM\": \"WP\", \"EA\": \"SP\", \"WA\": \"SP\"}\n",
    "    df[\"basin\"] = df[\"basin\"].fillna(df[\"subbasin\"].map(sub_to_basin))\n",
    "\n",
    "# Select authoritative agency tracks\n",
    "def select_agency_tracks_fast(df):\n",
    "    df = df.copy()\n",
    "    preferred_by_basin = {\n",
    "        \"EP\": [\"hurdat_epa\"],\n",
    "        \"NA\": [\"hurdat_atl\"],\n",
    "        \"WP\": [\"jtwc_wp\", \"jtwc_cp\", \"jtwc_ep\"],\n",
    "        \"NI\": [\"jtwc_io\"],\n",
    "        \"SI\": [\"jtwc_sh\", \"jtwc_io\"],\n",
    "        \"SP\": [\"jtwc_sh\"],\n",
    "        \"SA\": [\"jtwc_sh\"],\n",
    "    }\n",
    "    \n",
    "    df[\"use_for_modeling\"] = False\n",
    "    df[\"modeling_agency\"] = pd.NA\n",
    "    \n",
    "    if \"usa_agency\" not in df.columns:\n",
    "        print(\"Warning: usa_agency missing. Using all rows.\")\n",
    "        df[\"use_for_modeling\"] = True\n",
    "        df[\"modeling_agency\"] = \"all\"\n",
    "        return df\n",
    "    \n",
    "    for basin, agencies in preferred_by_basin.items():\n",
    "        mask = (df[\"basin\"] == basin) & (df[\"usa_agency\"].isin(agencies))\n",
    "        df.loc[mask, \"use_for_modeling\"] = True\n",
    "        df.loc[mask, \"modeling_agency\"] = df.loc[mask, \"usa_agency\"]\n",
    "    \n",
    "    # Overwrite lat/lon with authoritative agency positions where available\n",
    "    if {\"usa_lat\", \"usa_lon\"}.issubset(df.columns):\n",
    "        pos_mask = df[\"use_for_modeling\"] & df[\"usa_lat\"].notna() & df[\"usa_lon\"].notna()\n",
    "        df.loc[pos_mask, \"lat\"] = df.loc[pos_mask, \"usa_lat\"].astype(float)\n",
    "        df.loc[pos_mask, \"lon\"] = df.loc[pos_mask, \"usa_lon\"].astype(float)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = select_agency_tracks_fast(df)\n",
    "\n",
    "# Build state columns dynamically (include metric columns if present)\n",
    "metric_cols = [\"x_km\", \"y_km\", \"vx_km\", \"vy_km\", \"lat_ref\", \"lon_ref\"]\n",
    "metric_cols_present = [c for c in metric_cols if c in df.columns]\n",
    "\n",
    "base_state_cols = [\"sid\", \"iso_time\", \"lat\", \"lon\", \"v_lat\", \"v_lon\", \"storm_speed\", \"storm_dir\"]\n",
    "state_cols = base_state_cols + metric_cols_present\n",
    "\n",
    "if \"usa_wind\" in df.columns:\n",
    "    state_cols.append(\"usa_wind\")\n",
    "\n",
    "# Build feature columns (only keep those that exist)\n",
    "feature_cols = [\n",
    "    \"track_curvature\", \"latitude_regime\", \"hemisphere\", \"motion_regime\",\n",
    "    \"storm_age_hours\", \"day_of_year\", \"month\",\n",
    "    \"beta_drift_lat\", \"beta_drift_lon\",\n",
    "    \"v_lat_smooth\", \"v_lon_smooth\",\n",
    "    \"v_lat_avg_6h\", \"v_lon_avg_6h\",\n",
    "    \"v_lat_avg_12h\", \"v_lon_avg_12h\"\n",
    "]\n",
    "\n",
    "optional_features = [\"storm_stage\", \"dist2land\", \"is_approaching_land\", \"land_gradient\"]\n",
    "feature_cols.extend([c for c in optional_features if c in df.columns])\n",
    "\n",
    "# Extra columns for bookkeeping\n",
    "extra_cols = [\"basin\", \"nature\", \"use_for_modeling\", \"modeling_agency\", \"usa_agency\"]\n",
    "extra_cols = [c for c in extra_cols if c in df.columns]\n",
    "\n",
    "# Build clean dataset\n",
    "df_clean = df[state_cols + feature_cols + extra_cols].copy()\n",
    "df_clean = df_clean.dropna(subset=[\"lat\", \"lon\"]).reset_index(drop=True)\n",
    "\n",
    "# Save full dataset with all engineered columns\n",
    "df.to_pickle(\"hurricane_paths_processed_FULL.pkl\")\n",
    "print(\"Saved full feature df -> hurricane_paths_processed_FULL.pkl\")\n",
    "\n",
    "# Save clean dataset\n",
    "df_clean.to_pickle(\"hurricane_paths_processed_CLEAN.pkl\")\n",
    "print(\"Saved clean modeling df -> hurricane_paths_processed_CLEAN.pkl\")\n",
    "\n",
    "# Save modeling-only dataset (authoritative-agency subset)\n",
    "df_model_clean = df_clean[df_clean[\"use_for_modeling\"]].copy()\n",
    "df_model_clean = df_model_clean.sort_values([\"sid\", \"iso_time\"]).reset_index(drop=True)\n",
    "df_model_clean.to_pickle(\"hurricane_paths_processed_MODEL.pkl\")\n",
    "print(\"Saved modeling-only df -> hurricane_paths_processed_MODEL.pkl\")\n",
    "\n",
    "print(f\"\\nState cols: {state_cols}\")\n",
    "print(f\"Metric cols present: {metric_cols_present}\")\n",
    "print(f\"df_clean rows: {len(df_clean):,}\")\n",
    "print(f\"df_model_clean rows: {len(df_model_clean):,}\")\n",
    "print(f\"storms total: {df_clean['sid'].nunique():,}\")\n",
    "print(f\"storms modeling: {df_model_clean['sid'].nunique():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d42b806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset: 721,960 observations\n",
      "Unique storms: 13,450\n",
      "Missing values in metric state variables: 0\n",
      "Missing values in degree state variables: 0\n",
      "Date range: 1842-10-25 03:00:00 to 2025-11-23 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Validate data quality\n",
    "print(f\"Processed dataset: {len(df_clean):,} observations\")\n",
    "print(f\"Unique storms: {df_clean['sid'].nunique():,}\")\n",
    "# Check for metric coordinates\n",
    "metric_cols = ['x_km', 'y_km', 'vx_km', 'vy_km']\n",
    "has_metric = all(col in df_clean.columns for col in metric_cols)\n",
    "\n",
    "if has_metric:\n",
    "    print(f\"Missing values in metric state variables: {df_clean[metric_cols].isnull().sum().sum()}\")\n",
    "    print(f\"Missing values in degree state variables: {df_clean[['lat', 'lon', 'v_lat', 'v_lon']].isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(f\"Missing values in state variables: {df_clean[['lat', 'lon', 'v_lat', 'v_lon']].isnull().sum().sum()}\")\n",
    "print(f\"Date range: {df_clean['iso_time'].min()} to {df_clean['iso_time'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2f70d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset\n",
    "df_clean.to_pickle(\"hurricane_paths_processed.pkl\")\n",
    "df_clean.to_csv(\"hurricane_paths_processed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a1122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a686c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22876cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
